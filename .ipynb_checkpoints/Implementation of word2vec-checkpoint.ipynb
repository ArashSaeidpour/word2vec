{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of word2vec on Stanford Sentiment Treebank (SST) dataset\n",
    "“You shall know a word by the company it keeps” (J. R. Firth)\n",
    "## Introduction\n",
    "This notebook is a step by step guide on implementation of word2vec skipgram on Stanford Sentiment Treebank (SST) dataset, and is the solution to coding sections of [Assignment #2](http://web.stanford.edu/class/cs224n/assignments/a2.pdf) of Stanford's [\"CS224n: Natural Language Processing with Deep Learning\"](http://web.stanford.edu/class/cs224n/) course. Contents of this notebook are taken from the course materials. <br>\n",
    "I recommend reading the original papers [1,2] and all the course materials on the word2vec (specially this [one]( http://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes01-wordvecs1.pdf) before proceeding to implementation. But if you are looking a for a shortcut, the [this link](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/) covers all the major points in both papers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conda environment\n",
    "First you need to create a conda virtual environment with all the necessary packages to run the code. Run the following command from within the repo directory to create a new env named \"word2vecenv\": "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "conda env create -f env.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activate the \"word2vecenv\" that you just created:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "source activate word2vecenv (or conda activate depending on your OS and anaconda version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installing the IPython kernel in your env:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "conda install ipykernel\n",
    "ipython kernel install --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now switch your notebook's kernel to \"word2vec\" env."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding negative sampling\n",
    " The original word2vec paper [1] proposed \"Naive softmax loss\" as objective function ($J$):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$- \\sum^{2m}_{j=0,j \\neq m} u^T_{c-m+j}v_c + 2m \\log \\sum_{k=1}^{|V|} \\exp(u_k^T v_c) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in which $v_c$ is the output word vector of the center word, $u_j$ is input word vector of outside word $j$, $|V|$ is the vocabulary size and $m$ is windows size. Note that everytime we update or evaluate $J$ we need to do a summation over the entire vocabulary (sum of $|V|$ terms), whihc is in order of millions and computationally huge! That's why author of the original paper came up with the idea of \"Negative sampling loss\" [2] to approximate the softmax normalization term (Sigma in the abvoe equation). The idea is that rather than looping over the entire vocabulary to do the summation, we generate negative samples and use them to estimate the objective function. We will use the latter in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a pair $(w, c)$ of word and context. Did this pair come from the training data? Let’s denote by $P(D = 1|w, c)$ the probability that $(w, c)$ came from the corpus data. Correspondingly, $P(D = 0|w, c)$ will be the probability that $(w, c)$ did not come from the corpus data. First, let’s model $P(D = 1|w, c)$ with the sigmoid function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P(D = 1|w, c,\\theta) = \\sigma (v_c^T v_w) = \\frac{1}{1+exp(-v_c^T u_w)} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and naturally if the pair did not come from the corpus, we will have:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P(D = 0|w, c,\\theta) = 1 - P(D = 0|w, c) =1 - \\sigma (v_c^T v_w) = 1- \\frac{1}{1+exp(-v_c^T u_w)} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For every training step, instead of looping over the entire vocabulary, we can just sample several negative examples! We \"sample\" from a noise distribution ($P_n(w)$) whose probabilities match the ordering of the frequency of the vocabulary. For a given center word (vector), $v_c$, and outside (context) word, $u_o$, and $K$ negative samples, $\\tilde{u}_k^T$, our objective function for Skip-gram model will be:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$J_{neg-sample} (v_c,u_o,U) = -\\log \\sigma(u^T_{o}v_c) - \\sum_{k=1}^{K} \\log \\sigma (-\\tilde{u}^T_{k}v_c) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in which $U$ is the matrix of outside words. We will need partial derivatives of $J_{neg-sample} (v_c,u_o,U)$ wrt to $v_c$,$u_o$ and $u_k$ to for backpropagation (try to work out these derivatives from $J_{neg-sample} (v_c,u_o,U)$): "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\partial J_{neg-sample} (v_c,u_o,U) / \\partial v_c = -(1 - \\sigma(u^T_o v_c))u_o + \\sum_{k=1}^{K} (1-\\sigma(-u_k^Tv_c)) u_k$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\partial J_{neg-sample} (v_c,u_o,U) / \\partial u_o = - (1- \\sigma (u_o^T v_c))v_c$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\partial J_{neg-sample} (v_c,u_o,U) / \\partial u_k =  (1- \\sigma (-u_k^T v_c))v_c$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use these derivatives to implement *negSamplingLossAndGradient* function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from utils.treebank import StanfordSentiment\n",
    "from utils.gradcheck import gradcheck_naive\n",
    "from utils.utils import normalizeRows, softmax\n",
    "import pickle\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import glob\n",
    "import os.path as op\n",
    "\n",
    "# Check Python Version\n",
    "import sys\n",
    "assert sys.version_info[0] == 3\n",
    "assert sys.version_info[1] >= 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following command line code to fetch the \"Stanford Sentiment Treebank (SST): dataset:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sh get_datasets.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take the data for a spin!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the dataset first and see what's inside!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11855"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.numSentences()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 11855 sentences in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19539"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset.tokens())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and 19539 'tokens'. \"dataset.tokens()\" is mapping from tokens(words) to indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4316"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.tokens()['python']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is the index of 'python' in our dictionary!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Naive softmax implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid function\n",
    "Good ol' sigmoid function which we will use to calculate the loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    x -- A scalar or numpy array.\n",
    "    Return:\n",
    "    s -- sigmoid(x)\n",
    "    \"\"\"\n",
    "    sig_x=1/(1+np.exp(-x))\n",
    "\n",
    "    return sig_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negative sampler:\n",
    "We are going to define *getNegativeSamples* to draw random negative samples from the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNegativeSamples(outsideWordIdx, dataset, K):\n",
    "    \"\"\" Samples K indexes which are not the outsideWordIdx \"\"\"\n",
    "\n",
    "    negSampleWordIndices = [None] * K\n",
    "    for k in range(K):\n",
    "        newidx = dataset.sampleTokenIdx()\n",
    "        while newidx == outsideWordIdx:\n",
    "            newidx = dataset.sampleTokenIdx()\n",
    "        negSampleWordIndices[k] = newidx\n",
    "    return negSampleWordIndices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negative sampling loss and gradient:\n",
    "We are going to use $\\partial J_{neg-sample} (v_c,u_o,U) / \\partial v_c$, $\\partial J_{neg-sample} (v_c,u_o,U) / \\partial u_o$ and $\\partial J_{neg-sample} (v_c,u_o,U) / \\partial u_k$ that we derived above to implement calculate the loss and gradient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negSamplingLossAndGradient(\n",
    "    centerWordVec,\n",
    "    outsideWordIdx,\n",
    "    outsideVectors,\n",
    "    dataset,\n",
    "    K=10\n",
    "                               ):\n",
    "    \"\"\" Negative sampling loss function for word2vec models\n",
    "    \"\"\"\n",
    "\n",
    "    negSampleWordIndices = getNegativeSamples(outsideWordIdx, dataset, K)\n",
    "    indices = [outsideWordIdx] + negSampleWordIndices\n",
    "    \n",
    "    u_ws=outsideVectors[indices,:]\n",
    "    u_ws[1:,:]=-u_ws[1:,:]\n",
    "    sigmoid_uws=sigmoid(u_ws@centerWordVec.reshape(-1,1)).squeeze()\n",
    "    loss= -np.log(sigmoid_uws).sum()\n",
    "    \n",
    "    \n",
    "    gradCenterVec=(sigmoid_uws[0]-1)*u_ws[0,:]\n",
    "    for row in range(1,u_ws.shape[0]):\n",
    "        gradCenterVec=gradCenterVec-(1-sigmoid_uws[row])*u_ws[row,:]\n",
    "        \n",
    "        \n",
    "    gradOutsideVecs=np.zeros(outsideVectors.shape)\n",
    "    gradOutsideVecs[indices[0],:]=((sigmoid_uws[0]-1)*centerWordVec).reshape(-1,)\n",
    " \n",
    "    for i,idx in enumerate(indices[1:]):\n",
    "        gradOutsideVecs[idx,:]=gradOutsideVecs[idx,:]+((1-sigmoid_uws[i+1])*centerWordVec).reshape(-1,)\n",
    "   \n",
    "\n",
    "    return loss, gradCenterVec, gradOutsideVecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skipgram\n",
    "Given a minibatch including a center word and a list of outside words form the dataset, we will implement the *skipgram* function to calculate the loss and gradients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skipgram(currentCenterWord, windowSize, outsideWords, word2Ind,\n",
    "             centerWordVectors, outsideVectors, dataset,\n",
    "             word2vecLossAndGradient=negSamplingLossAndGradient):\n",
    "    \"\"\" Skip-gram model\n",
    "\n",
    "    Arguments:\n",
    "    currentCenterWord -- a string of the current center word\n",
    "    windowSize -- integer, context window size\n",
    "    outsideWords -- list of no more than 2*windowSize strings, the outside words\n",
    "    word2Ind -- a dictionary that maps words to their indices in\n",
    "              the word vector list\n",
    "    centerWordVectors -- center word vectors (as rows) for all words in vocab\n",
    "                        (V in pdf handout)\n",
    "    outsideVectors -- outside word vectors (as rows) for all words in vocab\n",
    "                    (U in pdf handout)\n",
    "    word2vecLossAndGradient -- the loss and gradient function for\n",
    "                               a prediction vector given the outsideWordIdx\n",
    "                               word vectors, could be one of the two\n",
    "                               loss functions you implemented above.\n",
    "\n",
    "    Return:\n",
    "    loss -- the loss function value for the skip-gram model\n",
    "            (J in the pdf handout)\n",
    "    gradCenterVecs -- the gradient with respect to the center word vectors\n",
    "            (dJ / dV in the pdf handout)\n",
    "    gradOutsideVectors -- the gradient with respect to the outside word vectors\n",
    "                        (dJ / dU in the pdf handout)\n",
    "    \"\"\"\n",
    "\n",
    "    loss = 0.0\n",
    "    gradCenterVecs = np.zeros(centerWordVectors.shape)\n",
    "    gradOutsideVectors = np.zeros(outsideVectors.shape)\n",
    "\n",
    "    idx_vc=word2Ind[currentCenterWord]\n",
    "    idx_uws=[word2Ind[outsideWord] for outsideWord in outsideWords]\n",
    "    vc=centerWordVectors[idx_vc,:].reshape(-1,1)\n",
    "    \n",
    "\n",
    "    for idx_uw in idx_uws:\n",
    "        loss_uw, gradCenterVec_uw, gradOutsideVecs_uw = negSamplingLossAndGradient(vc,idx_uw,outsideVectors,dataset)\n",
    "        loss=loss+loss_uw\n",
    "        gradCenterVecs[idx_vc,:]= gradCenterVecs[idx_vc,:] + gradCenterVec_uw.reshape(1,-1)\n",
    "        gradOutsideVectors= gradOutsideVectors + gradOutsideVecs_uw\n",
    "        \n",
    "\n",
    "    return loss, gradCenterVecs, gradOutsideVectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also define a helper function to sequentially draw samples and perform stochastic gradient decent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2vec_sgd_wrapper(batchsize,word2vecModel, word2Ind, wordVectors, dataset, \n",
    "                         windowSize,\n",
    "                         word2vecLossAndGradient=negSamplingLossAndGradient):\n",
    "    loss = 0.0\n",
    "    grad = np.zeros(wordVectors.shape)\n",
    "    N = wordVectors.shape[0]\n",
    "    centerWordVectors = wordVectors[:int(N/2),:]\n",
    "    outsideVectors = wordVectors[int(N/2):,:]\n",
    "    for i in range(batchsize):\n",
    "        windowSize1 = random.randint(1, windowSize)\n",
    "        centerWord, context = dataset.getRandomContext(windowSize1)\n",
    "\n",
    "        c, gin, gout = word2vecModel(\n",
    "            centerWord, windowSize1, context, word2Ind, centerWordVectors,\n",
    "            outsideVectors, dataset, word2vecLossAndGradient\n",
    "        )\n",
    "        loss += c / batchsize\n",
    "        grad[:int(N/2), :] += gin / batchsize\n",
    "        grad[int(N/2):, :] += gout / batchsize\n",
    "\n",
    "    return loss, grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Decent:\n",
    "Takes a function (f) and an input vector (x0) and performs gradient decent. we also define two other functions; *save_params* to save the matrix of word vectors every $n$ iterations while training and *load_saved_params* to load saved word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_params(iter, params):\n",
    "    params_file = \"saved_params_%d.npy\" % iter\n",
    "    np.save(params_file, params)\n",
    "    with open(\"saved_state_%d.pickle\" % iter, \"wb\") as f:\n",
    "        pickle.dump(random.getstate(), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_saved_params():\n",
    "    \"\"\"\n",
    "    A helper function that loads previously saved parameters and resets\n",
    "    iteration start.\n",
    "    \"\"\"\n",
    "    st = 0\n",
    "    for f in glob.glob(\"saved_params_*.npy\"):\n",
    "        iter = int(op.splitext(op.basename(f))[0].split(\"_\")[2])\n",
    "        if (iter > st):\n",
    "            st = iter\n",
    "\n",
    "    if st > 0:\n",
    "        params_file = \"saved_params_%d.npy\" % st\n",
    "        state_file = \"saved_state_%d.pickle\" % st\n",
    "        params = np.load(params_file)\n",
    "        with open(state_file, \"rb\") as f:\n",
    "            state = pickle.load(f)\n",
    "        return st, params, state\n",
    "    else:\n",
    "        return st, None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(f, x0, step, iterations, PRINT_EVERY=10,SAVE_PARAMS_EVERY = 5000,ANNEAL_EVERY = 20000,useSaved=False):\n",
    "    \"\"\" Stochastic Gradient Descent\n",
    "\n",
    "    Implement the stochastic gradient descent method in this function.\n",
    "\n",
    "    Arguments:\n",
    "    f -- the function to optimize, it should take a single\n",
    "         argument and yield two outputs, a loss and the gradient\n",
    "         with respect to the arguments\n",
    "    x0 -- the initial point to start SGD from\n",
    "    step -- the step size for SGD\n",
    "    iterations -- total iterations to run SGD for\n",
    "    postprocessing -- postprocessing function for the parameters\n",
    "                      if necessary. In the case of word2vec we will need to\n",
    "                      normalize the word vectors to have unit length.\n",
    "    PRINT_EVERY -- specifies how many iterations to output loss\n",
    "\n",
    "    Return:\n",
    "    x -- the parameter value after SGD finishes\n",
    "    \"\"\"\n",
    "    if useSaved:\n",
    "        start_iter, oldx, state = load_saved_params()\n",
    "        if start_iter > 0:\n",
    "            x0 = oldx\n",
    "            step *= 0.5 ** (start_iter / ANNEAL_EVERY)\n",
    "\n",
    "        if state:\n",
    "            random.setstate(state)\n",
    "    else:\n",
    "        start_iter = 0\n",
    "    \n",
    "    x=x0\n",
    "    exploss=0\n",
    "    for iter in range(start_iter + 1, iterations + 1):\n",
    "        loss = None\n",
    "        grad=0\n",
    "        loss,grad=f(x)\n",
    "        x=x-step*grad\n",
    "        \n",
    "        if iter % PRINT_EVERY == 0:\n",
    "            if not exploss:\n",
    "                exploss = loss\n",
    "            else:\n",
    "                exploss = .95 * exploss + .05 * loss\n",
    "            print(\"iter %d: %f\" % (iter, exploss))\n",
    "\n",
    "        if iter % SAVE_PARAMS_EVERY == 0:\n",
    "            save_params(iter, x)\n",
    "\n",
    "        if iter % ANNEAL_EVERY == 0:\n",
    "            step *= 0.5\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Showtime: Training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(314)\n",
    "dataset = StanfordSentiment()\n",
    "tokens = dataset.tokens()\n",
    "nWords = len(tokens)\n",
    "\n",
    "# A 10 dimensional vector, Google's word2vec has 300 features.\n",
    "dimVectors = 10\n",
    "\n",
    "# Context size: How far away from the center word look for outside words?\n",
    "C = 5\n",
    "max_windowSize=C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordVectors = np.concatenate(\n",
    "    ((np.random.rand(nWords, dimVectors) - 0.5) /\n",
    "       dimVectors, np.zeros((nWords, dimVectors))),\n",
    "    axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(31415)\n",
    "np.random.seed(9265)\n",
    "\n",
    "startTime=time.time()\n",
    "\n",
    "batch_size=50\n",
    "wordVectors = sgd(\n",
    "    lambda vec: word2vec_sgd_wrapper(batch_size,skipgram, tokens, vec, dataset, C,\n",
    "        negSamplingLossAndGradient),\n",
    "    wordVectors, 0.3, 42000, PRINT_EVERY=1000,SAVE_PARAMS_EVERY = 5000,ANNEAL_EVERY = 20000,useSaved=True)\n",
    "\n",
    "endTime=time.time()\n",
    "\n",
    "print(\"Training time: %d minutes\" %((endTime - startTime)/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "I am going to use PCA to project word vectors onto 2D space and plot them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordVectors = np.concatenate(\n",
    "    (wordVectors[:nWords,:], wordVectors[nWords:,:]),\n",
    "    axis=0)\n",
    "\n",
    "visualizeWords = [\n",
    "    \"great\", \"cool\", \"brilliant\", \"wonderful\", \"well\", \"amazing\",\n",
    "    \"worth\", \"sweet\", \"enjoyable\", \"boring\", \"bad\", \"dumb\",\n",
    "    \"annoying\", \"female\", \"male\", \"queen\", \"king\", \"man\", \"woman\", \"rain\", \"snow\",\n",
    "    \"hail\", \"coffee\", \"tea\"]\n",
    "\n",
    "visualizeIdx = [tokens[word] for word in visualizeWords]\n",
    "visualizeVecs = wordVectors[visualizeIdx, :]\n",
    "temp = (visualizeVecs - np.mean(visualizeVecs, axis=0))\n",
    "covariance = 1.0 / len(visualizeIdx) * temp.T.dot(temp)\n",
    "U,S,V = np.linalg.svd(covariance)\n",
    "coord = temp.dot(U[:,0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZQAAAEACAYAAACUMoD1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nOzdd3yOVxvA8d/JnkZsQqlVaouE2CR2UYSipUXRUlVao160NWq3qNaoVK3abYPU1lTtVVqbhixqRJE82TnvH088Eklq5EkicX0/n/fjee773Oc+d7x15T7rUlprhBBCiIyyyO4GCCGEyB2ssrsBQjyKclUlccIuS24WQbQO0cFZci8hchkJKOLZ54Qd3YnMknv9gGOW3EeIXEi6vIQQQpiFBBQhhBBmIQFFCCGEWUhAETnTZlz5jF1Zfq0QIl0SUIQQQpiFzPISOZkVn/MlcVTBir/xYQg/8w5ReKOxw4YjDGcE1sAKqnKJWSiisOVQdjdciNxI3lBEzpVIWYqxgnF4obiHH2/SkO8YSxvG0QyNHYvxBuASX+DKWMbSPptbLUSuJW8oIudShPEmhwEoxAb+oS9HCWIb76KxR5OPSM5xnANo8tKHAwCUYj3naZadTRciN5I3FJGTPbwRneYGn1OH/oyjOQ6sJBE7ElFplBVCmJkEFJFzaUrwPbUBuEFH7JPGRioQzmkciKItALW5i+Iu31EHgCA6ZU+DhcjdVE7cbbhgwYK6dOnS2d0MkUWiE6KxcEj5u09sdCyXz1zGMY8jhnsGbOxsKFm+JNdDr3Pn5h2sba2xsbHB2taaIqWKEBURRcjFEJSFwjmfM3du3aFCzQqp7pVoSMTOMmu2DRMiqx09evSm1rpQZtWfI8dQSpcuzZEjR7K7GSKLnA88j1NBpyy5V8TNCCqUSR1ohMgNlFJXMrN+s3R5KaVaKaXOKaUuKqVGpXHeVim1Oun8QaVU6aTj1kqp75VSfyqlziilRpujPUIIIbJehgOKUsoSmAe0BioD3ZVSlR8q1he4rbUuB3wBTE067gPYaq2rArWBAfeDjRBCiJzFHG8o7sBFrfXfWutYYBXQ4aEyHYDvkz6vA5orpe7PvHFUSlkB9kAscNcMbRJCCJHFzBFQSgDJExKFJB1Ls4zWOh64AxTAGFwigatAEDBDax2e1k2UUv2VUkeUUkdu3LhhhmYLIYQwJ3MMyqs0jj08dSy9Mu5AAlAcyA/sUUrt0Fr/naqw1guBhQBubm45b2qaeGp2NnZE3IxI81zYP2HExMWY7V621rYZut7Oxo5SJUqZqTVC5CzmCCghQMlk312BsHTKhCR1b+UFwoEewBatdRxwXSm1F3ADUgUU8fx61D/QWTUD7HGkF/iEeB6Yo8vrMFBeKVVGKWUDvAb4PVTGD+id9LkLsEsbF8AEAc2UkSNQFzhrhjYJIYTIYhkOKEljIoOBrcAZYI3W+pRS6jOl1P2N+BYDBZRSF4FhwP2pxfMAJ+AvjIHpO631yYy2SQghRNYzy8JGrbU/4P/QsXHJPkdjnCL88HURaR0XQgiR88heXkKkYdG8RUQZokzfyxcrn42tESJnkIAixEMSEhL49ptviYqKenRhIYSJBBSRq3z95dcs/mYxAONHjcennbFHdc+ve3iv33v8tPYnmtdtTjOPZkwaN8l0Xfli5Zk+cTrtmrZjzvQ5/HP1H3za+tClbRdTmSmfTcHL04t2zdpx47qshRLiYRJQRK7i4enBwf0HATh5/CSGCANxcXEc3n+YMmXLMGn8JNZsWsO2vdv449gfbNm0BQBDpIGKlSuyafcmPhj1AUWKFWHt5rWs27zOdL5WnVrs2LeDuvXrsmLJimx7RiGeVRJQRK5SrWY1/vzjTyLuRWBja0Nt99qcOHaCg/sPkidvHuo1qEeBggWwsrKiU9dOHNhrTOJoaWlJ2w5t063XxsYG71bGbMJVa1QlJCgkS55HiJxEAorIVaytrXEt5crq5atxc3fD3dOdfXv2cSXwCiVKPrwj0AO2drZYWlqme97K2grj9nPG4BMfH2/2tguR00lAEblOXc+6zJ87H4/6Hnh4erDMdxkvV32ZWnVqcWDvAcJvhZOQkMBP636iXoN6adbh5ORExD1Z9S7Ek5CAInIdd093rl+7jpu7G4UKF8LW1hZ3T3eKFC3C6PGj8Wnrg7enN1WrV6Vl25Zp1tHzzZ683vn1FIPyQoj/liNTALu5uWnJ2Cgga7M5Pg7J+CieZUqpo1prt8yqX95QhBBCmIUEFCGEEGZhlr28hMjJQsJCzJZTxXDbYJZ67pP8KiInkYAinnsxcTE4FnA0T2XKvPlZJL+KyEkkoIgc7b+yOT4uw21D2jlFn6Y91nbmqUiIHEgCisjRzNUd9CzNFBMip5JBeSGEEGYhAUUIIYRZSEARQghhFhJQhEjH7OmzaVirId3ad+Pdt95l/pz5dGnThRPHTgAQfiscjyoegDEp14T/TaBN4zZ41fNime8yUz3fzP7GdHzGpBkABF8JprFbYz567yOaujele4fuktBL5HgSUIRIw8njJ/Fb78e237fx7fJvTUEkPT8s/QHnPM74B/iz+dfNrPx+JUGXgwjYGUDgpUA2/7qZbXu3cfKPk6Yt8wMvBdL77d7sPrSbPPny4P+zf1Y8mhCZRmZ5CZGGg/sO0qpdK+wd7AHwbuP9n+UDdgVw5q8zbP55MwD37t4j8FIgAbsCCNgVQIsGLQAwRBgIvBRICdcSlHyhJFWqVQGgWo1qBAcFZ+ITCZH5JKAIkY77+U+Ss7SyJDExEYDo6OgHJzRMnD6RJl5NUpT/deevDB42mDf6vJHiePCVYGxtbR/Ua2lJdFQ0QuRk0uUlRBrq1q/Llk1biIqKIuJeBNt/2Q5AyVIlOfnHSQA2/7TZVL5x88YsXbyUuLg4AC5duIQh0kCT5k1YvWw1kRGRAFwNu8rNGzez+GmEyBpmeUNRSrUCZgOWwLda6ykPnbcFlgK1gVtAN6315aRz1YAFQB4gEaijtZZf1US2qlqjKq90eoUW9VvgWtIVD0/j4PvAIQMZ2Hsg61etp36j+qbyPXr3IDgomFYNW6G1xqWgC74rfWncvDEXzl2gvVd7ABwcHZi7aO5/ZocUIqfKcD4UpZQlcB7wBkKAw0B3rfXpZGXeBapprQcqpV4DXtVad1NKWQHHgDe01ieUUgWAf7XWCf91T8mHIszpcXKqzJw8E0cnRwYOGZhFrTKS/CrCnHJCPhR34KLW+m+tdSywCujwUJkOwPdJn9cBzZWxg7oFcFJrfQJAa33rUcFECCHEs8kcXV4lgOTTU0IAj/TKaK3jlVJ3gAJABUArpbYChYBVWutpad1EKdUf6A9QqpRs5y2y1vCPh2d3E4R45pkjoKS1T+vD/WjplbECGgB1AAOwM+mVbGeqwlovBBaCscsrQy0WIoPMmUMlLbbWtrgWd820+oXIDOYIKCFAyWTfXYGwdMqEJI2b5AXCk44HaK1vAiil/IFaQKqAIsSzxKw5VNIQeSsy0+oWIrOYI6AcBsorpcoAocBrQI+HyvgBvYH9QBdgl9b6flfXCKWUAxALNAa+MEObhHhsT5NTxZw5VNKrP8IxAjsbya8ico4MB5SkMZHBwFaM04Z9tdanlFKfAUe01n7AYmCZUuoixjeT15Kuva2UmoUxKGnAX2u9Oc0bCZFJnjanSmbmUIlwkNldIucxyzoUrbU/4P/QsXHJPkcDPulcuxxYbo52CCGEyD6yUl4IM7jz7x2WLFqS3c0QIltJQBHCDO7eucvSb5dmdzOEyFayOaQQZjB5/GSuBF7Bu743jZo2omChgmzcsJHY2FhatWvFh2M+BKBP9z6EhYYREx1D33f68vpbr2dzy4UwH3lDEcIMPv70Y14o8wLb926nUdNG6eZAmTlvJlt+24J/gD++830JvxWezS0XwnzkDUUIM0svB0rd+nXxne/LL5t+ASAsNIzAS4G4FHDJzuYKYTYSUIQwM611mjlQ9u3Zx55f97Bxx0bsHezp0qYLMTGZt9peiKwmXV5CmIGjkyMREcbFkenlQLl39x558+XF3sGei+cvcuzwsexsshBmJ28oQpiBSwEX6njUoZlHM5p6N6WjT8dUOVCaeDVh2eJleNXz4sXyL1KrTq1sbrUQ5pXhfCjZQfKhiOz2ODlUMkLyoIjMkBPyoQghhBASUIQQQpiHBBQhhBBmIQFFCCGEWcgsLyGewtPkUHnS+oXIaSSgCPEUnjaHihC5mXR5CSGEMAsJKEIIIcxCAooQQgizkIAihBDCLCSgCCGEMAsJKEIIIcxCAooQQgizkIAihBDCLMwSUJRSrZRS55RSF5VSo9I4b6uUWp10/qBSqvRD50sppSKUUh+aoz1CCCGyXoYDilLKEpgHtAYqA92VUpUfKtYXuK21Lgd8AUx96PwXwC8ZbYsQQojsY443FHfgotb6b611LLAK6PBQmQ7A90mf1wHNlVIKQCnVEfgbOGWGtgghhMgm5ggoJYDgZN9Dko6lWUZrHQ/cAQoopRyBkcCnj7qJUqq/UuqIUurIjRs3zNBsIYQQ5mSOgKLSOPZwXuH0ynwKfKG1fuS2rVrrhVprN621W6FChZ6imUIIITKTOXYbDgFKJvvuCoSlUyZEKWUF5AXCAQ+gi1JqGpAPSFRKRWutvzJDu4QQQmQhcwSUw0B5pVQZIBR4DejxUBk/oDewH+gC7NJaa6Dh/QJKqU+ACAkmQgiRM2U4oGit45VSg4GtgCXgq7U+pZT6DDiitfYDFgPLlFIXMb6ZvJbR+wrxOIJCg4iOjc7uZpidnY2d5GQRzxyzJNjSWvsD/g8dG5fsczTg84g6PjFHW4RILjo2GqeCTtndDLPLzGyRQjwtWSkvhBDCLCSgCCGEMAsJKEIIIcxCAooQWeTEsROM/WhsdjdDiExjlkF5IcSjVa9Vneq1qmd3M4TINBJQxCPl1Km3djZ26Z7r070PYaFhxETH0Pedvrz+1uuUL1aevu/0ZceWHdjZ2fHdqu8oVLgQQwcOxdnZmRPHT3Dj+g3GfDaGdh3bobVm4tiJ7N6+G6UUQz4aQofOHXjv7fdo17EdLdu2BGBw38G079weJ2cn5s+Zz9K1S5k5eSahIaEEXQ4iNCSUfu/0o+87fQH4YuoX/LjmR4q7FsfFxYVqNasxcMjALPmZCZERElDEI+XUqbf/NbV25ryZ5HfJT1RUFG2btKVN+zYYIg3UqlOLUeNGMXHsRFYsWcHQEUMB+Oeff/hp209cPH+Rt7q9RbuO7fD38+fUn6fYvm874bfCadOkDXXr16VH7x4smreIlm1bcvfOXY4cOsKXC77k0P5DKdpw8fxF1m5eS2REJA1rNaRXv16c/vM0/n7+bP19KwnxCbRs2JJqNatl6s9JCHORMRTxXPKd74uXpxevNH+FsNAwAi8FYmNjg3crbwCq1qhKSFCIqXyrtq2wsLCgwksVuL856aH9h+jYpSOWlpYUKlyIuvXrcuLYCeo1qMflvy9z88ZNflr3E23at8HKKvXvbs1bNsfW1haXAi4ULFSQG9dvcGj/IVq2aYm9vT1Ozk54t/bOmh+IEGYgbyjPucfpzgoMCcQh0iGLWvT0bK1tcS3u+shy+/bsY8+ve9i4YyP2DvZ0adOFmJgYrKytSMqqgKWlJfHx8aZrbGxtTJ+NuwY9+DMtnV/rzIbVG/Bb78fMr2em3V5bW9NnS0tLEuIT/rNOIZ51ElCec4/TneVgcMDRxTGLWvT0Im9FPla5e3fvkTdfXuwd7Ll4/iLHDh97qvvVrV+X5b7L8enhw7+3/+XgvoOMnWicxdW1Z1faNm1L4cKFqVip4mPX6V7PnZFDRzJ4+GAS4hPYuXUnPd/s+VTtEyKrSZeXeCphIWF0bdv1keW+mf0NB/YdAKD/6/05/edpANo1bce/t/8F4M1ubz51O/w2+HHjnyfLj9PEqwkJ8Ql41fNi2sRp1KpT66nu3fqV1lSqUglvT2+6tuvKmM/GULhIYQAKFS5E+Qrl6fr6o39GydWoXYMWrVvg7elNv579qF6zOs55nJ+qfUJkNZUTX7Hd3Nz0kSNHsrsZucL5wPOPfEO5FHQp1RtKWEgYQwcMZc3mNelel5CQgKWlpel7/9f7M3TkUCpXrUy7pu1YvmE5+fLny1D7k9cZeSuSsi+UNZ27PyifHRMKogxRNK/bnC17tpAnb54nujYyIhJHJ0eiDFF0at2JabOnUbVG1RRlIm5GUKFMBXM2WTwHlFJHtdZumVW/dHmJpxafEM+4EeM4d+YcpUqXYsK0CXRp04UOnTuwf+9+uvXsxr49+2jYtCFerbzSrad+jfrs/WMvhkgDH7z7Affu3iM+Lp53P3iXJs2bEBYSxntvv0eN2jU4eewkhYoUYtY3s/j919859dcpxnw4Bls7W+Z9PS8Lnz59v+3+jeGDhtN/UP8nDiYAI4aM4Py588REx+DTwydVMBHiWSUBRTy1K4FXGD95PNVrVefT0Z+yduVawDiA7fuDL2AcAP8vb73ylumzra0tM7+aiZOzE//e/pfePr1p3KwxAEGXg5g8azJjJ45l5Psj2bV1F206tGHN8jUp3lCeBY2aNuLw6cNPff0832cjMArxpCSgiKdWpFgR08rv1u1bs2rZKgBatGnx2HV8t/E76teoDxhnTc2bNY9jR46hLBQ3rt/g1s1bABR3LW4a3K70ciXCQh9OCiqEyG4SUESakq8kb9+tPd0HdKdBuQZ0fbMrB/ccxNbOlrjoOPp37s+10Gu80uMVlFLEx8UzasAo4mLjAChQogAA30z7htOHTjPmnTFEGaKIMBjHNxqUawBOcGTfESaPmkxUdBT2tvZUqlaJe3fuERsTy5G9R/jn4j/06dCHl6q+xKHfD+HdSdZnCPGskYAi0pR8Jbl3fW9a+7QmyhBFbc/aDPnfEAZ1H0R4aDhT5kzB2dGZAd0G0GdYH87+dZbp306ncNHCBP0dxJvt36RDtw68M+Idjp88Tv/B/Zk+ZjraNvVkkKvBV2np05JxU8bRtVlXrgVfIzYmlnmfz6NY2WL4/uzLx+98nOIaB0cHIiPT7+qys7HLlcmo/mtbGSGyiwQUkSbf+b78sukXAK5fu05wYDDWNtZ4NvUE4IVyL3Du3Dm2bN7CiaMnMNwz4NPdhx+W/MDM8TMJvBCIpYUl9+7cM9WpteabKd/Q8+2eLFywMNU9q9SqQuDlQHr59EIrTcFCBQm5EkKREkWIxrj4smXHlpw/c950zSudXmHy+MnpDspLmlwhsk6OCCjKVZXECdOvZC/bv8z5wPP/dYlZPW/5ux9eSd62WVvjSnKrByvJ8+bNS6+3e9HrnV6AsevKzt6ODu07YIg08Pk3n5OYmIhnGU/TDK/aNWoTfjOc9q+1p/1r7U332/vHXo7sO4KDowOLli4CYOrHU6lUvRIFCxXE1taWpRuWmsqXKFWCAUMGAMbtS5q3bA48/sJGIUTmyBEBBSfs6I7pXwuLbRZZurYgN3aZ/JeHV5KfPnn6sa+NuBdB4WKFsbCwYNOaTSQkJADw2/bfOPjbQRauT/1m8l9KlytN6JVQwoLDKF6yONv8tj3R9UKIrJMzAorIUk28mrBs8TK86nnxYvkXqVyt8mNf69Pbh4/e/ogdm3bg5umGvYM9AMsXLOfGPzfo1cb4RtOoRSPeGfHOI+uzs7dj1OejGNxjMPlc8lGlZpWneyghRKbLESvl1UuqfPI3lKrbqoZu2bIly+6fm1clP+1K+axkiDTg4OiA1popH0+hVJlS9Oyfen+rtFbK59a/NyGeRmavlDfLXl5KqVZKqXNKqYtKqVFpnLdVSq1OOn9QKVU66bi3UuqoUurPpD+bmaM9Inf5ccWPdPfqjk8THyLuRtDp9U7Z3SQhRBoy3OWllLIE5gHeQAhwWCnlp7VO3vHeF7ittS6nlHoNmAp0A24Cr2itw5RSVYCtQImMtulR9u3ZZ8qcJx7NztqOyPDsG/Du2KUjHbt0NH1PjE4kMjp1ewy3DUQ4Phjvkqm1QmQtc4yhuAMXtdZ/AyilVgEdgOQBpQPwSdLndcBXSimltT6erMwpwE4pZau1jjFDu4SZlCiW6THeLCIcpItLiOxkji6vEkBwsu8hpH7LMJXRWscDd4ACD5XpDBx/3GASfCWYRrUb8eHgD2nm0YzBfQfz2+7f6ODdgfo16nP8yHGOHzlOe6/2tGjQgvZe7bl44WKqegyRBoa9O4w2jdvQokELtm7e+nhPLYQQIgVzvKGoNI49PNL/n2WUUi9j7AZLfxOob+nJPV4HuFXUuL/T5b8vs2DpAqbNmUabJm34ae1P/LTtJ7b5b2PuzLnMXjCbDVs2YGVlxW+7f2Pqp1NZtHxRimpnz5hN/Ub1mfX1LO78e4e2TdvSsElDHByf/QyF5pCbVpJLF5cQ2cscASUEKJnsuyvw8M5998uEKKWsgLxAOIBSyhX4Eeiltb6U7l36sQJYAVBgW4FQgJIvlKTSy5UAqPBSBRo0boBSipcqv0RwUDB3795l6MChBF4KRClFXFxcqmp/2/Ub2/23M3/ufABiYmIIDQmlfMXyT/yDyImepwWbQojMZY6Achgor5QqA4QCrwE9HirjB/QG9gNdgF1aa62UygdsBkZrrfc+6Y2T5+S2sLAw5f22sLAgIT6B6ROn49nQk8UrFxN8JZgubbukqkNrzcLlCylXvtyT3l4IIUQyGR5DSRoTGYxxhtYZYI3W+pRS6jOl1P39NRYDBZRSF4FhwP2pxYOBcsBYpdQfSf8rnNE23Xfv7j2KFi8KwJoVaWcWbNy8Md/N/47763H+OvGXuW4vhBDPFbOslNda+wP+Dx0bl+xzNOCTxnUTgYnmaENa3nn/HYYOHMrCrxZSv1H9NMsMHTGU8aPG41XPC601rqVcZTqxEEI8BVkp/xhkxbUQIjfIESvlhRBCCAkoQgghzEICihBCCLOQgCKEEMIsckY+lAii+QHT/umJ9olZurpbVmDnLEGhQUTHRmfZ/Z63jJ5CpCdHBBQdopPvFYabm5vMuhImDweQwJBAHPJn3dY5hhBDpgcwCVoiJ8gRAUWI/xIdG50iSZiDwSFrE4IpMj0ldW7Zb03kbjKGIoQQwiwkoAghhDAL6fISOUpaA+6BIYE4RD4YMwkOC8Y+0j7D9xr74Vh69+9NuQrl6P9Gf2Z8NYM8efOkKhf1b5QpGYOttS2uxV0zfG8hciIJKCJHeXi8BFKPmdhH2+OQ78kG5Tet24StnS3e7bxNxywcLbDLZ4eDiwMWDhbYu9jjkDeNei3AsYDx/pG3HqQm7tKmC2MnjqV6reopiq9esZqTx04yaeakJ2qjEM86CSgi11uzYg02tjZ07NKRb+Z+Q+DFQKbNnsaxI8fY7r8dr1ZebN++nfj4eA4ePMjw0cOxd8j4G44QzxsZQxG51k7/nQx5Ywhb1mzhx2U/kpCQgN8SP4IvBDOw60Bmjp1JkaJF+GHpD9SoUoMmjZtQ/qXyfPvVt7zf630uHLvAolmLuHf3HvGx8Yx8e6Sp7tCgUAb1GATAmqVreKPzG3Rt25XpE6eTfMPV9avX096rPc08mnH8yPFUbbx18xZvv/42bRq3oU3jNhw+cDjzfzBCZBIJKCJXCgoMImBrADO/m8nC9Qu5c+cOW37agtaa8pXL897Y97CwtuDi6YtcuXyFrf5b2bB6Azu27CDglwD6vt+X8rXKU9y1OMsXLMfKxgoHRwcunTMmFd328zZavGLMWN26Y2uWrV/Gms1riI6OZvuW7aZ2RBmi8Nvhx+RZkxk+aHiqdo4bMY63B72Nf4A/i5Yv4sPBH2bND0iITCBdXtkoq1d0Q+5cIHc19Cofd/6YNbsfJFE7fvA4F85c4P2e7wOQGJfIvoB9KAuFV1svTv5xkti4WBJiE8hvnZ9yVctRplIZWndqzUCfgVSrXQ2WgHsjd1YuWAlAs3bN2PbzNvoP70/AtgBmL5sNwJ9//Mn0sdOJio7i32v/4lbTjRatjcGmQ5cOANStX5d79+5x5987Kdq+59c9nD933vQ94l4EEfcicHLO3HUtQmQGCSjZKK0B5sz23CyQ0+D9ijd93usDwLLFy9j6y1asrK2oWr0qC+ctpHCxwjg5OxF8PRin/Ma/h9joWBISEtKssm6jumxYtoHqdapTvnJ58ubLS0xMDIu+XMTKn1ZStFhR5k6aS0x0jOkapVSKOh7+npiYiN8OP+ztZcxG5HwSUESuEB8fz7j3x3Hur3MULFaQnu/35JP3PuHgloMoFHkL5eXmPzexs7Dj/U7vE6NjuB52HdsEW0rlLcXOTTuxcbJhybQl2DrZ0rdFX27euMluu91UrV2V3/f+ztShU4m8FsmU4VOwt7HnTvgdLK0tAciXPx+GSAMBOwN4teOrpnb5bfCjfqP6HNp/iDx58qSadty4WWOWLFzCO++/A8BfJ/+iSrUqWfeDE8KMZAxF5ApXLl2hU89OrN65GgdHBw7vPkzs3VgSrBLAES6dvUTH1h1N5bv27MrgzwZTtHRRXAq4UMutFp27d6ZAwQK07dIWp6JOOOZz5NzRc/Ts35N6NepRu2FtPln4CTaONty5Zey6cnJ2wqutF93adWPYoGFUrFwxRbvy5ctHe6/2jBo6ihlfzUjV7gnTJ3Di+Am86nnRpE4Tlvkuy9wflBCZSN5QRK5QpHgRarjXAKBx68as/349L5R7gRk/GP8RP77vOBuXb+SnvT/Rq0kvGrVtRJESRWjo1ZAZI2YQZYgiTx7j20OH1zvQt2hfzv5xliVfLME5jzOnjpxi3Nfj2LN7D+26t2OT7ybTvXv27cnH4z4GjOtQyr5QFoB1/uvSbGu3nt3o1rMbAC4FXJi/ZH7m/FCEyGLyhvKMCb4STDOPZimOnTh2grEfjc2mFuUMD49NPIqd/YOUBBfPXSQ0KBSPxh4AWNtYA2BhaUFCvHE8RaOZPWk2OzftpGP3jqkrFELIG0pOUL1W9VSrrUVK10KvcfLISaq5VWPP1j3U9KyJ/yp/wq6EUfyF4uz8aSfV3KuleW25iuXwaOpB3nx5063/5dovU7hYYboO6MrRPUeJuPOcTJHbzogAACAASURBVG4Q4glIQHmGXQm8wttvvM2rPq+y//f9LF27lJmTZxIaEkrQ5SBCQ0Lp904/+r7TF4Avpn7Bj2t+pLhrcVxcXKhWsxoDhwzM5qfIGmXKl2HT2k1MGjmJgkUL8upbr/JSjZeY9N4kEhISqFC1Am26t3nq+l9/73WmfDCFAP8AqrlXw6WwC/aOMjNLiOTMElCUUq2A2YAl8K3WespD522BpUBt4BbQTWt9OencaKAvkAAM0VpvNUebcrqLFy7y7lvvMuvrWdy9c5f9v+9/cO78RdZuXktkRCQNazWkV79enP7zNP5+/mz9fSsJ8Qm0bNiSajVT/kYeEhbC7eu3s/pRMuRx1s0UK1GMdQEPxiuu/XONmJgYKlauyPSl003H46PiiY+KZ/5PxjELw78GAN79+F3T9+TnXEu68um8TzH8a0AlKMbMGoOllSXnTp7j+N7jpvpsrWzN+sxC5FQZDihKKUtgHuANhACHlVJ+WuvTyYr1BW5rrcsppV4DpgLdlFKVgdeAl4HiwA6lVAWtddoLAZ4Tt27eos9rfVi0fBEVK1Vk3559Kc43b9kcW1tbbG1tKVioIDeu3+DQ/kO0bNPStJ7Bu7V3qnpj4mJwcHHI8rUvGfE462bsrO2IDH+wKaOztTPO1s5mbUfw9WA+Gf4JiYmJWFtbM/qz0RS0L2g6f//+htsGIhzN3x0maahFTmCONxR34KLW+m8ApdQqoAOQPKB0AD5J+rwO+EoZR1E7AKu01jFAoFLqYlJ9+3mOOedxprhrcQ4fOEzFShVTnbe1ffAbsaWlJQnxCSn2j3relChWItPvUbZUWX499Osjy0U4REh6avHcMkdAKQEkz/keAnikV0ZrHa+UugMUSDp+4KFrM/9fh2ecjY0Nvit96fFqDxwdHSlSrMgjr3Gv587IoSMZPHwwCfEJ7Ny6k55v9kyzbOjVUKLjsnbLFzC+SWTFP/5CiOxhjoCS1nzNh39dTq/M41xrrECp/kB/gFKlctdeVGlxcHTg+zXf071Dd94f8f4jy9eoXYMWrVvg7emNa0lXqtesjnOetLt9ouOiszbnepLk3VJCiNzHHAElBCiZ7LsrEJZOmRCllBWQFwh/zGsB0FovBBYCuLm55dr+nZIvlGTXwV0A5M2XF/8AfwBatm0JwPCPU+5Ye78swMAhAxn+8XCiDFF0at2JAe8NyKJWZx07G7tnej8yGesQzzNzBJTDQHmlVBkgFOMge4+HyvgBvTGOjXQBdmmttVLKD1iplJqFcVC+PHDIDG16Lo0YMoLz584TEx2DTw8fqtaomt1NMrvctlOyELlJhgNK0pjIYGArxmnDvlrrU0qpz4AjWms/YDGwLGnQPRxj0CGp3BqMA/jxwKDnfYZXRszznffE13w/73tsbG3o3q87M8fP5Pzp8yxYu4BDew7ht9qPBs0b4DvXFzQ0aN6AIf8bAkCDcg3o+mZXDu45SJ68eRg0ehBzJs7hWug1hn86nMYtGxMWHMbY98YSZYgCYMjIIZQtVZZ9e/Yx6/NZ5C+Qn3Onz1GtRjXmfjv3iVe7CyGeLWbZekVr7a+1rqC1Lqu1npR0bFxSMEFrHa219tFal9Nau9+fEZZ0blLSdRW11r+Yoz3i8dWqW4vjh4yZBE+fOE1UZBRxcXH8cegPSpUpxZxJc1iwdgErt6/k1IlT7P5lN2BMHFXbszYrtq7AwcmBr6d+zbxV85ixeAbzpxvXcuQvkJ+vV33Nym0rmTJ/CnM+n2O6718n/+LTKZ/y6+FfuXL5imQqFCIXkJXyz7mXqr3E2ZNniYyIxMbGhpeqvsSZE2c4fvA4DVs0xM3TjfwF8gPQ+tXWHD94nKatm2JtY41nU08Ayr1UDhtbG6ytrSlXqRxhIcZhsPj4eKaNmca5U+ewtLDkyt9XTPetUbsGxUsUB+Dlai8TfCUY93ruWfz0QghzkoCSjbJ6gNlw24BLEZcUU4atra0pVrIYyxYs49z5c2hrzfABw4m6G0X1etXZ+9teOnh3YNKMSYSGhLJl8xYO/3GYhPgErly+Qukypblw7gIhwSEcP3mc4KBgoqOM9a9cuBKXgi6s2rGKxMREPMt4mu5rY2Nj+mxpYUl8QnyW/RyEEJlDAko2yo4BZqeCTlwKupTiWC2PWmxYvoG7kXfpO7gv498bT6xNLFcuX8HO2o63B73N4vmLibweyahPR9GsTTPqvViPebPmMX2ucWuTG9dv4LvOF2tba+q/WJ9rV68RcS+CwsUKY2FhwaY1m9LNhCiEyB1k+3pBTY+a3L55m2KlilGnXh1s7WxxLeNKo+aNGDx6ML4zfTmw7QCly5dm8+bNdG3blbjYOC5deBCYSr5QEqc8Ttja2qIsFNfCruHT24dNazfRu11vrvx9RdLcCpHLyRuKwL2hOz/t/4mhA4YC8OPvPzJ+5Hisbazxau9FdffqDB0wlBgdg5uHGzPnzSQsJIz+b/QHoEnbJpz584ypPs/WnsTHx1PqxVKs3rnadLzPQGN+d8+Gnng2fND9NWnmpKx4TCFEJpM3FPHYIiIiKFykMAAbN2zM5tYIIZ41ElDEY+vVtxdzZ87lrdfeIiFRxkOEECmpnLhLrZubmz5y5Eh2NyPHOR943jQon117eZUtVfaJrom4Kbv3CmEuSqmjWmu3zKpf3lCEEEKYhQzKP0fur3sxhBvS2dM5cxluG4hweLJ1N7LZohA5hwSU50jydS/ZkbUxwlG6r4TIzaTLSwghhFnIG0ouEhQaRHTsozMxhv0TRkxITBa0KCVb6wepi+1s7GQreiFyGQkouUh0bPRjdWVVKJj93U7PcpIsIcTTkS4vIYQQZiEBRQghhFlIQBFCCGEWElCeA4u/WUxjt8YM7js4U+qfOXkm8+fMz5S6hRA5hwzKPwe+//Z7lq9fTqnSMqtKCJF5JKDkciOHjiTochBvvfYW7Tu350rgFc6eOkt8QjzDRw+nZduWrF6xmq2btpKQkMC5M+cYMHgAsXGxrF+1HhsbG5atW0Z+l/ysWLKCFd+tIDYuljIvlmHOwjnYO6TMcXL578uMGT6GW7duYW9vz/S50ylXoVw2Pb0QIitJl1cuN/XLqRQpVoS1m9diMBio36g+/gH+rN20lgn/m4Ah0gDAudPnmLd4Hpt3b2bqhKnY29uz7fdt1Havzbof1gHQ+pXW+Af4s2PfDspVKMcPS39Idb8R749gwvQJbPltC2MnjmX0sNFZ+rxCiOwjbyjPkd92/cZ2/+3Mn2sc74iJiSE0JBQAz0aeODk74eTshHMeZ7xbewNQ6eVKnP7rNADnzpxj2oRp3L1zl8jISBo3b5yi/siISI4ePMqA3gNMx2JjYrPi0XKcx12Emh1k0al4WhJQniNaaxYuX0i58im7oI4dOYaNjY3pu4WFBba2xlXtykKREG/MffLBOx+weOViXq76MqtXrGb/nv0p6klMTCRP3jxs37s9k58k53vcRajZQRadiqeVoS4vpZSLUmq7UupC0p/50ynXO6nMBaVU76RjDkqpzUqps0qpU0qpKRlpi3i0xs0b893877ifA+evE3890fUR9yIoUrQIcXFx/Ljmx1TnnfM4U/KFkmz80ZjNUWvNqT9PZbzhQogcIaNjKKOAnVrr8sDOpO8pKKVcgPGAB+AOjE8WeGZorV8CagL1lVKtM9ge8R+GjhhKXHwcXvW8aObRjGkTpz3R9R/97yPaNWtH9w7dU73l3PfVt1+xaukqvDy9aOrelG2bt5mj6UKIHCBDGRuVUueAJlrrq0qpYsCvWuuKD5XpnlRmQNL3BUnlfnio3GzgL631okfdVzI2pu1+Rsac4HnPxPgs/1097383uVlmZ2zM6BhKEa31VYCkoFI4jTIlgOBk30OSjpkopfIBrwCz07uRUqo/0B+gVCkZMBTPr1s3b9HbpzexcbFMmDaB6/9cZ8akGRQqUoh1m9dld/PEMyTV5A9bbNVLqrzZbxRBtA7RwY8MKEqpHUDRNE6NecxbqTSOmV6LlFJWwA/AHK313+lVorVeCCwE4xvKY95biHRl50yrwJBAHAwOqY7bWdtRoliJNK544Pdff6dshbLMXmD8/avnqz2ZPGsy9RvVz5S2ipwr1eQPOxLpTqTZb/QDjvAYbyhaa6/0ziml/lFKFUvW5XU9jWIhQJNk312BX5N9Xwhc0Fp/+RjNFsJssnOmlUOkA44ujimObVq7ie+/+h5bW1sqvVyJkWNHMmzQMMJvhuNS0IUvvv6C27dvM3HcRKKjovGu703rdq05dOAQQUODaNGmBR9/+jGTx09m/579xMbG0vvt3rzR5w0Avpn9DRs3bCQ2NpZW7Vrx4ZgPs+PRRS6W0S4vP6A3MCXpz5/TKLMVmJxsIL4FMBpAKTURyAv0y2A7hMjRLp27hO9sX+Z+P5fa1WtzO/w2QwcOpctrXejasyurlq1i7Iix+P7gy4djPuTksZNMmjkJgH179jF24liq16rO8u+W45zHGf8Af2JiYujYoiONmzUm8FIggZcC2fzrZrTWvNntTQ7sPUDd+nWz+clFbpLRgDIFWKOU6gsEAT4ASik3YKDWup/WOlwpNQE4nHTNZ0nHXDF2m50FjimlAL7SWn+bwTY9t+xs7HLMGgI7G7vsbsIz5fDvh2nerjn58ucDIL9Lfo4eOsq3K4z/OXR+rTMTx058ZD0BuwI489cZNv+8GYB7d+8ReCmQgF0BBOwKoEWDFgAYIgwEXgqUgCLMKkMBRWt9C2iexvEjJHvr0Fr7Ar4PlQkh7fEV8ZRkdXPOpbUm6ZeqdD3qvLEimDh9Ik28mqQ4/OvOXxk8bLCp+0s8pxKw5jN2MY5mj1X+S97AkijeYx2f8wV52cG7bE6vuOzlJUQW27dnH3+e+DPFMfeG7mzfuJ07/94B4Hb4bdw83Ph5nbEXecOaDbjXc39k3Y2bN2bp4qXExcUBcOnCJQyRBpo0b8LqZauJjDCOx14Nu8rNGzfN+VgiNxrKMt7jsacOytYrQmSx/Xv2E5UQRd1mD7qbylYsS58hfRjy5hDs7OyoUq0KE6ZNYNigYcyfM980KP8oPXr3IDgomFYNW6G1xqWgC74rfWncvDEXzl2gvVd7ABwcHZi7aC4FCxXMtOcUzyxLPmc6cbhhwTV68BYb6cRdXkdjjSWX6cp7lCea6QzHgkiG81gJjzK0sDG7yMJGYQ5pLS40RBoY0HsAV8OukpiQSOfXOnP8yHG+XfEtWzdv5d233uVMyBkSExNp6t6U/Sf3p7tl/62btxg1dBShwcYNOD+d+ilFixXlleavkGiViEsJF0aOHUlNt5qm+0eGR1K2VNks/Tk8TBY25h4P/3+8RPESZ4igAhVpTXdOMZn5OLMNN3ZTj9sATGcEVtzgA75LEVD+q8vrBxz1WX1B3lCESGb3jt0ULVaUZeuWAXD3zl2W+Ro/H9x3kIqVKnLi2Ani4+OpWdsYCEa8P4IpX0zhxXIvcuzwMUYPG83aTWsZN2Icbw96G/d67oQGh9Lj1R4EHAngjT5vEJUQRb8PZHKjyAYWBNEd4yZ7tpwkhpKcoSI7GIkmDxpHbFMs7XhsElCESOalyi8x4X8TmDRuEl6tvPDw9KD0i6W5cO4Cfxz9g/6D+3Ng7wESEhJw93T/zy379/y6h/PnzpuOR9yLIOJezpiFJ3K15DklEtFYEcyXVKAP3TnNHLoSSb2nqVgCihDJlC1fll8CfmHXtl18/snnNG7WGI96HuzavgsraysaNm3I0IFDSUxIZOyksf+5ZX9iYiJ+O/ywt7dP405CPFOcKME/3MCKu3TCkqtPU4nM8hIimWtXr2HvYE/n1zozcMhA/jzxJx71Pfj262+pXac2BQoW4Hb4bS5euEjFShX/c8v+xs0as2ThElPdf500pgtwdHYkKjIqy59NiHTlZRq72cwCVmHFxaetRt5QhEjm7KmzTBw7EWWhsLay5vMvPqdCpQrcvH7TtAiwcpXKXP/numldyFfffsXoD0Yze/ps4uPi6dC5Ay9XfZkJ0yfw8fCP8arnRXx8PB71PZj65VS8W3nzVs+32PPbHj4Y8QHValYz3d9w20CEQ/Z2i8mi01zMkjjG8SBNSMrZW0tTlf+ImabPo/ngUdXLLC/x3HoWt5CXGVbCnFLN8ipZ4iQfYP68UzLLS+RGT7KDcHo7/prL4+wcLERuIgFF5CpPsoNwWjv+mlNkuPl3CRfiWSYBJQfKyjwedjZ2skeYEOKxSEDJgbIyj0dO2b1YCJFaqh3Io7G4nwzLrCKIBgko4jlma21L5K3M65Z6mhlbMsNKmFOq3oUYYvRZfSGz7icBRTy3XIu7Zuj6v07+xT9X/6F5y1QZHACIcJQZW+L5IgsbnzMfDv6Q82fPP7rgcyY+Pv6Jrzn15yl2bduVCa0RImeSN5RcSGuN1hoLi9S/L8z4akY2tCj7fTH1C35c8yPFXYvj4uJCtZrV2LFlB7U9anPkwBG823jj090n1e7AderW4fiR44wfNZ7o6Gjs7OyY9c0sSr1QihmTZhAdFc2hA4cYPGwwHTp3yOanFCJ7SUDJJYKvBPN659fxbOjJ0cNHebnqy5w9fZboqGjadmjLh2M+BKBLmy6m/OPli5Wn7zt92bFlB3Z2dny36jsKFS6UzU9ifieOncDfz5+tv28lIT6Blg1bmlan3/33Lut/WQ/AoD6D0twduFyFcmzYsgErKyt+2/0bUz+dyqLli1LldhfieScBxcyyYkpvYEggDpEpF+RdDbvKxcsXGTZ+GIM+GoSjnSP5XfKTkJBAt1e6cfqv01SuUjnFNYZIA7Xq1GLUuFFMHDuRFUtWMHTE0Exte3Y4tP8QLdu0NG3S6N3a23Sufef2ps/p7Q589+5dhg4cSuClQJRSpmyIQoiUJKCYWVZM6XUwpF6Q5xDlQPEXi+PR1IPIW5Hs+mUXK5asICE+gX+u/cOFsxdSBRQbGxu8Wxn/ca1aoyp7du/J1HZnl//aXsjB4UFgTm934P999D88G3qyeOVigq8E06Vtl0xrqxA5mQzK5yL3/yEMCw1jwZwFrPZbzY79O2jesjnRManfmqysrUwbHFpaWj7VwHRO4F7Pne1bthMdHU1kRCQ7t+5Ms1x6uwPfu3uPosWLArBmxRrTeScnJyIiZJ2OEPdJQMmFDJEG7B3tyZM3Dzeu32D39t3Z3aRsVaN2DVq0boG3pzf9evajes3qOOdxTlVuwvQJnDh+Aq96XjSp08SUqfGd99/h808+p4N3BxISEkzlPRt6cuHsBbzre/Pz+p+z7HmEeFZJl1cuVK5COapUq0JT96aUKl2KOnXrZHeTst3AIQMZ/vFwogxRdGrdiQHvDaDnmz1TlHEp4ML8JfNTXevm4cbvx383fR8xdgQA+V3y4x/gn7kNFyIHyVBAUUq5AKuB0sBloKvW+nYa5XoD/0v6OlFr/f1D5/2AF7XWVTLSnudZcdfirNn8oDvmy/lfpllunf860+cLVx8smG3XsR3tOrbLvAZmsxFDRnD+3HliomPw6eFD1RpVs7tJQuQ6GX1DGQXs1FpPUUqNSvo+MnmBpKAzHnADNHBUKeV3P/AopToB0hEtMtU833nZ3QQhcr2MBpQOQJOkz98Dv/JQQAFaAtu11uEASqntQCvgB6WUEzAM6A+sIRcyRBoY0HsAV8OukpiQyPsj3mfy+Mn4dPdh+5btxMfFs2DpAspVKMft8NsMHzScoMtB2NnbMW3ONCpXqUzzus3ZsGUDefLmoUrpKrz74bt0fqszY98bSzufdng08sjux8yVQsJCiImLeerrDbcNZmyNkez+LJ5lGQ0oRbTWVwG01leVUoXTKFMCCE72PSTpGMAEYCbwyP/ylFL9MQYeSpXKOf9B7d6xm6LFirJsnXGA9+6du0wePxmXAi5s3bOVJYuWMH/OfGZ8NYOZk2dSpVoVfH/w5feA33l/wPts37sdt7puHD5wGNdSrrxQ+gVOHjtJ57c68+exPxk9ZXQ2P2HuFRMXg2OBDGzMqjD7FHLZ/Vk8yx4ZUJRSO4CiaZwa85j3UGkc00qpGkA5rfUHSqnSj6pEa70QWAjGFMCPee9s91Lll5jwvwlMGjcJr1ZeeHga3yZatzdm4axWoxq/bPwFgEMHDrFo2SIAGjRuwO3w29y9cxePeh4c3HeQkOAQevXrxeIFi7l+9Tp58uXBwTHzMg7mRKm2684Aw21D2v/vfdy2WMvOweL58siAorX2Su+cUuofpVSxpLeTYsD1NIqF8KBbDMAVY9dYPaC2UupyUjsKK6V+1Vo3IRcpW74svwT8wq5tu/j8k89p3KwxALa2toBx/UdCvHEqaloL8JRSeNT3YMmiJbiWdGXkuJGsX7OeHZt3UNO9ZtY9SA5h7u6gZy3nvBDPsox2efkBvYEpSX+mNRl/KzBZKZU/6XsLYHTSmMo3AElvKJtyWzABuHb1Gvny56Pza51xdHJMsTDuYXU967JhzQY+GPkB+/bsw6WAC855nHHO40z4rXDi4uJ4ocwL1HSrydJ5Sxk6ZmiaaWYNtw1EOJrnt3TJzyGEeFwZDShTgDVKqb5AEOADoJRyAwZqrftprcOVUhOAw0nXfHZ/gP55cPbUWSaOnYiyUFhbWfP5F5/Tv1f/NMsOGz2MYe8Ow6ueF3b2dimm/tZ0q0liQiIA3q28+XrW17R/pT0uBVxS1RPh8Gzk4cjKVMX3ZfWg9czJM3F0cmTgkIEZqif5pp1C5FQZCiha61tAquxCWusjQL9k330B3/+o5zKQK9egNPFqQhOvJimOHfzroOlz9VrVTWtD8rvk57tV36VZz9xFc02f63jUIeROiPkba2ZZmar4Phm0FiL7yNYrQjyh2dNn07BWQ7q178alC5cA4xvGiWMnAAi/FY5HFePki9UrVtOnex96d+1N3ap1+W7Bdyz4agEtGrSgXbN23A5/sA54/er1tPdqTzOPZhw/cjzrH0yIDJKAIsQTOHn8JH7r/dj2+za+Xf6tKYj8l3OnzzFv8Tw2797M1AlTsbe3Z9vv26jtXpt1PzzYuSDKEIXfDj8mz5rM8EHDM/MxhMgUElCEeAIH9x2kVbtW2DvY45zHGe823o+8xrORJ07OThQoWMB4TVI+lkovVyI46MESrQ5djBkf69avy71797jz753MeQghMokEFJHjzJw8k/lzUm/imFXub/mfnKWVJYmJxkkT0dEpJyLY2NiYPltYWJimjCsLZZoynla9ad1HiGeZ7DZsZuZcWJeRNuQET7u1Sfi9cKISorh05VKqc+bc7iQwJBAHQ8qFoyXKlmDpmKW08WlDQkIC/hv9ae/Tnrwuedm1axd5CudhzbI1xMfHcynoEjdu3SDSkHpqd1r8NvhRv1F9Du0/RJ48eciTN4/ZnkWIrCABxcxkn6VHW7tyLQvmLiCGGCrWqMigoYP45ONP+Df8X/K55OPTzz+laPGiXA29muZxG2cbbB1t094WxYzbnThEps6MWatBLVp1akW/rv0o6lqU2vVqY+Ngw1vvv8WogaPY8csO6jSog7JQOLo4YuNoQ3zi4yUuy5cvH+292hNxL4KZ82aa5RmEyErqv9KjPqvc3Nz0kSNHsrsZ4hHOB55P9Y/7uTPn6NezHz9v/5nbEbeJt4xn/MjxNG/ZnFc6vcLP634mYFcAs76exdABQ9M8vmDOAhwcHXij7xup7hkZHknZUmXN0v5LVy5lbC+vJOZsU8TNZ2ONkciZlFJHtdZumVW/jKGILLU3YC9tO7Y1LcjMmy8vJ/84SetXjHubte3Qlj+O/gGQ7nEhxLNJAorIUlpr1CN2XExvMFoGqYV4tklAEVmqQZMGbPxxI+G3jLvv3Pn3DtVqVmPr5q0A+G/0p0btGgDpHn9a5YuVf+Jrtv+ync6tO9P/jbS3y7mvXdN2/Hv736dtmhC5ggzKiyxVsVJFhnw4hC5tuhBnEUflWpUZ8b8RfPrxpyxdvNQ0+A6kezwraK3RWvPzup8ZNX4UderWybJ7C5FTSUARWa5rz6507dk1xaD3gqUL+H7e99jY2lC0eFFmjp/J+dPnWbB2AYf2HMJvtR9/HPwD37m+oKFB8wam+hqUa0D3ft3Zs2MP1lbWrNywkkKFCxF0OYhBfQeREJ+Qaj+1b2Z/w8YNG4mNjaVVu1Z8OOZDgq8E83rn1/Fs6MnRw0ep16Qex/ceJ/h8MA0aN6B02dKcO32OD0Z9ABjz1Hfv1d24cWdUIoZwA9aJ1qme13DbQISD7P4scj8JKOKZUatuLZYtWEb3ft05feI0cbFxxMXF8cehPyhVphRzJs1hxdYVOOd1ZlD3Qez+ZTdNWzclyhBF1VpVGTRqEDPGzGDFkhUMHTGUcSPH0atvL3x6+LBk4RLTfQJ2BhB4KZDNv25Ga82b3d7kwN4DlHAtwaULl5j19Sw+/+JzAE4dO2XaBXj1itVcc7hG2ReMM7YcbRwpXrg4ZV8oizXWlHYtnfbuz44yM0s8H2QMRTwzXqr2EmdPniUyIhIbGxuq1q7KmRNnOH7wOE55nXDzdCN/gfxYWVnR+tXWHD9o3EDR2saaht4NAahQuQIhQcadmA8fOExHn44AdH6ts+k+AbsCCNgVQIsGLWjZsCWXzl8i8FIgAK6lXKntXjsrH1uIXEPeUMQzw9rammIli+G3yo9qbtUoX7k8R/YdIeRKCEVLFOXsybNpXmdlZWWaAWZpaUl8/IOFhGnNDNNaM3jYYN7ok3IdS/CVYBwc0k+pbGVpZdpeBSAm5slX+QuRm8kbinim1PKoxbL5/2/vbmKjqMM4jn9/feOlrYRXJdJW1CJCatKm1Z6EBA7iAT1AAokJJvXgVU8k3kw8qAe8eNCDCWLEkgaFmGAj1YMHNYImKEZeQhRKWy0tBfuqdB8PO2CtWzrKvOzL87nszs5/N7882ewzszPznwO0tLfQ/FgzXe92dQXomAAAA9tJREFUsW7jOppamjj55UmuDl1lenqa7o+6aWlvue1ntbW3caQrexPRw4cO33p985bNdB7oZGw0OyVKf18/VwavzJutrqGO09+fJpPJcLn3sl8X49wsBXmlvKRB4Je0c/xPK4D5f72KQTmVVNxmo6WSKhaR+cdrUyzmOvUsY4ByRhjifhYyQjXDTHAXEywP3jtGLb8BMMhDrOQMAOPUMEU11VximirGqQdEJdeYYhVL+AGASVbwJzcPeGRYzEXAGGcttZy9lWeUB1hIHxVMYMA49WRYRBmTGBUsYIBKxrjOw9RwljL+nu3xpknKmCKu3ZnS+T7dGa9TVoOZrYzrwwuyoRQySSfinPqgkGi9GtlN7pkT93GMF9iWcKR4HKTafrJzcXy0f5/C8Tolw//ycs45FwlvKM455yLhDSV5b6cdoCDU8l7aEQqEf5/C8TolwI+huNTc9hhKMYnxGIpz+cSvQ3HpGWWSg9z5DUfy3SiT8w9yrvD5HopzzrlI+DGUmElaJulTSeeCx6VzjPtE0oikj5POmCZJT0g6I+m8pL051i+Q1Bms/1rSfcmnTF+IOj0u6VtJNyTtSCNj2kLU6EVJP0o6JalHUkMaOYuZN5T47QV6zKwR6AmWc3kd+Pc9bYuYpHLgTWAbsAHYLWnDrGEdwFUzexDYB7yabMr0hazTReBZ4P1k0+WHkDX6Dmg1s0eALuC1ZFMWP28o8XsK2B883w88nWuQmfUAvycVKk88Cpw3swtm9gfwAdl6zTSzfl3AFpXerRvnrZOZ/Wxmp2DWzAOlI0yNPjez8WDxK2BNwhmLnjeU+N1tZv0AweOqlPPkk3uBSzOWe4PXco4xsxvANQimXykdYepU6v5rjTqAY7EmKkF+llcEJB0H7smx6qWksxSYXHsas88SCTOm2HkN5he6RpKeAVqBTbEmKkHeUCJgZlvnWifpV0mrzaxf0moIJjR0kN2KrJuxvAbom2NMr6QKYAkwnEy8vBGmTqUuVI0kbSW7obfJzPz+AxHzv7zidxTYEzzfAxxJMUu++QZolLRWUhWwi2y9ZppZvx3AZ1Z657qHqVOpm7dGkpqBt4DtZuYbdjHw61BiJmk5cAioJ3smzk4zG5bUCjxvZs8F474A1gM1wBDQYWbdKcVOjKQngTeAcuAdM3tF0svACTM7KmkhcABoJrtnssvMLqSXOB0h6tQGfAgsBSaBATPbmF7i5IWo0XGgCegP3nLRzLanFLcoeUNxzjkXCf/LyznnXCS8oTjnnIuENxTnnHOR8IbinHMuEt5QnHPORcIbinPOuUh4Q3HOOReJvwD1PKQd5Ua8kQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "plt.figure()\n",
    "for i in range(len(visualizeWords)):\n",
    "    plt.text(coord[i,0], coord[i,1], visualizeWords[i],\n",
    "        bbox=dict(facecolor='green', alpha=0.1))\n",
    "\n",
    "plt.xlim((np.min(coord[:,0]), np.max(coord[:,0])))\n",
    "plt.ylim((np.min(coord[:,1]), np.max(coord[:,1])))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "- 1- https://arxiv.org/pdf/1301.3781.pdf\n",
    "- 2- http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:word2vecenv]",
   "language": "python",
   "name": "conda-env-word2vecenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
